{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading cmudict: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed\n",
      "[nltk_data]     (_ssl.c:749)>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "nltk.download('cmudict')\n",
    "import pickle\n",
    "from HMM_sol import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in pickled preprocessed data\n",
    "\n",
    "file = open('../data/sonnets.pkl', 'rb')\n",
    "sonnets = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open('../data/shakespeare_dics.pkl', 'rb')\n",
    "dic_to_ids, dic_to_syl, ids_to_dic, syl_to_dic = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open('../data/dic_to_meter.pkl', 'rb')\n",
    "dic_to_meter = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open('../data/meter_to_dic.pkl', 'rb')\n",
    "meter_to_dic = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "file = open('../data/shakespeare_rhymes.pkl', 'rb')\n",
    "rhyme_data_raw = pickle.load(file)\n",
    "file.close()\n",
    "\n",
    "# Update rhyming word set to only include words that rhyme, but also end on a stressed syllable\n",
    "# (these will go at the end of lines)\n",
    "temp_rhymes = []\n",
    "\n",
    "for rhyme_set in rhyme_data_raw:\n",
    "    temp_rhyme_set = []\n",
    "    for word in rhyme_set:\n",
    "        if int(dic_to_meter[word][-1]) == 1:\n",
    "            temp_rhyme_set.append(word)\n",
    "    temp_rhymes.append(temp_rhyme_set)\n",
    "\n",
    "rhyme_data_raw = temp_rhymes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_emission_sequential(num_syl, stress_pattern, dic_to_syl, dic_to_ids, dic_to_meter, A, O):\n",
    "    temp = np.array(O)\n",
    "    \n",
    "    emission = []\n",
    "    states = []\n",
    "    syllable_count = 0\n",
    "\n",
    "    # choose starting state\n",
    "    y_i = np.random.choice(range(len(O)), p=[1 / len(O) for i in range(len(O))])\n",
    "    \n",
    "    while syllable_count < num_syl:\n",
    "        states.append(y_i)\n",
    "        \n",
    "        # choose a word index\n",
    "        array = range(len(O[y_i]))\n",
    "        observation_index = np.random.choice(array, p=O[y_i])\n",
    "    \n",
    "        num_syl_ok = syllable_count + dic_to_syl[ids_to_dic[observation_index]] <= num_syl\n",
    "        stress_ok = num_syl_ok\n",
    "        if (len(stress_pattern) != 0) and num_syl_ok:\n",
    "            actual_stress = dic_to_meter[ids_to_dic[observation_index]].split(', ')\n",
    "            if type(actual_stress) != list:\n",
    "                actual_stress = [actual_stress]\n",
    "            for i in range(len(actual_stress)):\n",
    "                stress_ok = stress_ok and (int(dic_to_meter[ids_to_dic[observation_index]].split(', ')[i]) == int(stress_pattern[i]))\n",
    "            \n",
    "        # Make sure we don't start off with too many syllables, or don't ruin the stress\n",
    "        while (not num_syl_ok or not stress_ok):\n",
    "            observation_index = np.random.choice(array, p=O[y_i])\n",
    "\n",
    "            num_syl_ok = syllable_count + dic_to_syl[ids_to_dic[observation_index]] <= num_syl\n",
    "            stress_ok = num_syl_ok\n",
    "            if (len(stress_pattern) != 0) and num_syl_ok:\n",
    "                actual_stress = dic_to_meter[ids_to_dic[observation_index]].split(', ')\n",
    "                if type(actual_stress) != list:\n",
    "                    actual_stress = [actual_stress]\n",
    "                for i in range(len(actual_stress)):\n",
    "                    stress_ok = stress_ok and (int(dic_to_meter[ids_to_dic[observation_index]].split(', ')[i]) == int(stress_pattern[i]))\n",
    "\n",
    "                \n",
    "        word = ids_to_dic[observation_index]\n",
    "\n",
    "        syllable_count += dic_to_syl[word]\n",
    "        emission.append(dic_to_ids[word])\n",
    "        stress_pattern = stress_pattern[dic_to_syl[word]:]\n",
    "        \n",
    "        # change states\n",
    "        array = range(len(A[y_i]))\n",
    "        y_i = np.random.choice(array, p=A[y_i])\n",
    "\n",
    "    return emission, states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_haiku_test(test_HMM, stress_pattern, syllable_pattern, hmm_name):\n",
    "    poem = []\n",
    "    \n",
    "    assert len(stress_pattern) == len(syllable_pattern)\n",
    "    \n",
    "    for index in range(len(stress_pattern)):\n",
    "        temp_syl = syllable_pattern[index]\n",
    "        temp_stress = stress_pattern[index]\n",
    "        \n",
    "        assert (len(temp_stress) == 0) or (temp_syl == len(temp_stress))\n",
    "        \n",
    "        emission, states = generate_emission_sequential(temp_syl, temp_stress, dic_to_syl, dic_to_ids, dic_to_meter, test_HMM.A, test_HMM.O)\n",
    "        x = ' '.join([str(ids_to_dic[i]) for i in emission])\n",
    "        poem.append(x)\n",
    "    \n",
    "    with open('../models/general/{}_sample.txt'.format(hmm_name), 'w+') as f:\n",
    "        for line in poem:\n",
    "            f.write(line)\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        for line in poem:\n",
    "            line_syl = []\n",
    "\n",
    "            line_list = line.split()\n",
    "\n",
    "            for word in line_list:\n",
    "                line_syl.append(dic_to_syl[word])\n",
    "            f.write(\"{} {}\".format(line_syl, sum(line_syl)))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        for line in poem:\n",
    "            line_meter = []\n",
    "            line_list = line.split()\n",
    "\n",
    "            for word in line_list:\n",
    "                line_meter.append(dic_to_meter[word])\n",
    "            f.write(str(line_meter))\n",
    "            f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_state_args = [30]\n",
    "num_iter_args = [200]\n",
    "\n",
    "# Haiku\n",
    "# stress_pattern = [[], [], []]\n",
    "# syllable_pattern = [5, 7, 5]\n",
    "# name = 'haiku'\n",
    "\n",
    "# Iambic Haiku\n",
    "stress_pattern = [[0, 1, 0, 1, 0], [0, 1, 0, 1, 0, 1, 0], [0, 1, 0, 1, 0]]\n",
    "syllable_pattern = [5, 7, 5]\n",
    "name = 'iambichaiku'\n",
    "\n",
    "for hidden_state in hidden_state_args:\n",
    "    for num_iter in num_iter_args:\n",
    "        with open('../models/hmm_hpt_{}_{}.hmm'.format(hidden_state, num_iter), 'rb') as f:\n",
    "            test_HMM = pickle.load(f)\n",
    "            generate_haiku_test(test_HMM, stress_pattern, syllable_pattern, '{}_hidden_{}_iter_{}'.format(name, hidden_state, num_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
